#Name the components on this agent
a1.sources= r1
a1.sinks= k4
a1.channels= c1

#Describe/configure the source
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 200
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = 10.140.100.8:9092,10.140.100.13:9092,10.140.100.29:9092
a1.sources.r1.kafka.topics = TOPIC_LUDP_PUSH
a1.sources.r1.kafka.consumer.group.id = mbg-push-ludptest2
a1.sources.r1.kafka.consumer.auto.offset.reset = earliest


#a1.sinks.k2.type = null
#a1.sinks.k1.type = thrift
#a1.sinks.k1.hostname = 10.0.163.12
#a1.sinks.k1.port = 9099
#a1.sinks.k1.batch-size = 200

#Use a channel which buffers events in memory
a1.channels.c1.type= file
a1.channels.c1.checkpointDir = push_channel/checkpoint
a1.channels.c1.dataDirs = push_channel/data
a1.channels.c1.capacity = 500000
a1.channels.c1.maxFileSize = 414643507


#a1.sinks.k2.type  = file_roll
#a1.sinks.k2.sink.directory = /data/apps/flume-push/testdata
#a1.sinks.k2.sink.rollInterval=120
#a1.sinks.k2.sink.batchSize=100



a1.sinks.k4.type = elasticsearch
a1.sinks.k4.hostNames = 10.140.100.53:9200
a1.sinks.k4.indexName = push
a1.sinks.k4.indexType = kafka
a1.sinks.k4.batchSize = 500
a1.sinks.k4.ttl = 5d
a1.sinks.k4.serializer = org.apache.flume.sink.elasticsearch.ElasticSearchDynamicSerializer
a1.sinks.k4.clusterName = node-ali-ol
a1.sinks.k4.indexNameBuilder = org.apache.flume.sink.elasticsearch.SimpleIndexNameBuilder
a1.sinks.k4.client = rest


#a1.sinks.k3.type  = logger


#Bind the source and sink to the channel
a1.sources.r1.channels= c1
#a1.sinks.k1.channel= c1
#a1.sinks.k2.channel = c1
#a1.sinks.k3.channel = c1
a1.sinks.k4.channel = c1